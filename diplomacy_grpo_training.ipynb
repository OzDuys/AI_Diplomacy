{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Diplomacy GRPO Training with Qwen2.5-1.5B-Instruct\n",
    "\n",
    "This notebook implements online GRPO (Group Relative Policy Optimization) training for Diplomacy agents using the multi-turn framework from willccbb/verifiers.\n",
    "\n",
    "## Features:\n",
    "- **7-Agent Self-Play** with Qwen2.5-1.5B-Instruct\n",
    "- **Online Training** - RL agent learns by playing games\n",
    "- **Alliance Formation Rewards** - Diplomatic success metrics\n",
    "- **Batched Generation** - Efficient GPU utilization\n",
    "- **Full Game Episodes** - Complete Diplomacy games (1901-1910)\n",
    "\n",
    "**Requirements**: Colab Pro (24GB GPU memory recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "**Important**: You need to replace `YOUR_USERNAME` with your actual GitHub username in the git clone command below, or upload the AI_Diplomacy folder directly to Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install all required dependencies\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "\n",
    "# Core ML packages\n",
    "!pip install torch transformers accelerate datasets numpy scipy\n",
    "!pip install tensorboard wandb matplotlib seaborn\n",
    "\n",
    "# GRPO framework\n",
    "!pip install git+https://github.com/willccbb/verifiers.git\n",
    "\n",
    "# AI Diplomacy specific dependencies\n",
    "!pip install coloredlogs python-dotenv ujson tornado tqdm\n",
    "!pip install anthropic openai google-generativeai together\n",
    "!pip install json-repair json5 bcrypt pytest pylint\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")\n",
    "\n",
    "print(\"\\nüîÑ Cloning AI Diplomacy repository...\")\n",
    "# Clone AI Diplomacy repository (replace with your actual repo URL)\n",
    "!git clone https://github.com/YOUR_USERNAME/AI_Diplomacy.git\n",
    "%cd AI_Diplomacy\n",
    "\n",
    "print(\"üì¶ Installing AI Diplomacy package...\")\n",
    "# Install in development mode\n",
    "!pip install -e .\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Verify installation and check for issues\n",
    "print(\"üîç Verifying installation...\")\n",
    "\n",
    "# Check critical packages\n",
    "packages_to_check = [\n",
    "    'torch', 'transformers', 'accelerate', 'numpy', \n",
    "    'coloredlogs', 'diplomacy', 'ai_diplomacy'\n",
    "]\n",
    "\n",
    "for package in packages_to_check:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"‚úÖ {package} - OK\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå {package} - MISSING: {e}\")\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"\\nüñ•Ô∏è Hardware Check:\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    if torch.cuda.get_device_properties(0).total_memory / 1e9 < 15:\n",
    "        print(\"‚ö†Ô∏è Warning: Less than 15GB GPU memory. Consider using Colab Pro.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Training will be very slow on CPU.\")\n",
    "\n",
    "# Check if we're in the right directory\n",
    "import os\n",
    "if os.path.exists('ai_diplomacy'):\n",
    "    print(\"‚úÖ AI_Diplomacy directory found\")\n",
    "else:\n",
    "    print(\"‚ùå AI_Diplomacy directory not found. Check git clone step.\")\n",
    "\n",
    "print(\"\\nüîß If you see any MISSING packages above, re-run the installation cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 2. Troubleshooting & Verification\n",
    "\n",
    "Let's verify the installation and check for any issues before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_config"
   },
   "outputs": [],
   "source": [
    "# Import with error handling\n",
    "try:\n",
    "    from ai_diplomacy.grpo_trainer import TrainingConfig, DiplomacyGRPOTrainer\n",
    "    import logging\n",
    "    print(\"‚úÖ Successfully imported GRPO training modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Please ensure all dependencies are installed and the repository is cloned correctly.\")\n",
    "    raise\n",
    "\n",
    "# Configure training parameters\n",
    "config = TrainingConfig(\n",
    "    # Model settings\n",
    "    model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    max_length=2048,\n",
    "    \n",
    "    # Training settings  \n",
    "    batch_size=7,  # One batch = one full game (7 agents)\n",
    "    learning_rate=1e-5,\n",
    "    num_episodes=50,  # Start with 50 episodes for proof of concept\n",
    "    max_year=1906,    # Shorter games for faster iteration\n",
    "    num_negotiation_rounds=2,  # Reduced for speed\n",
    "    \n",
    "    # GRPO specific\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    kl_coeff=0.1,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_every=10,\n",
    "    checkpoint_dir=\"/content/checkpoints\",\n",
    "    \n",
    "    # Logging\n",
    "    log_level=\"INFO\",\n",
    "    log_alliance_analysis=True,\n",
    "    \n",
    "    # Seeds for reproducibility\n",
    "    random_seed=42,\n",
    "    torch_seed=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training Configuration:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Episodes: {config.num_episodes}\")\n",
    "print(f\"  Max Year: {config.max_year}\")\n",
    "print(f\"  Learning Rate: {config.learning_rate}\")\n",
    "print(f\"  Batch Size: {config.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "initialize"
   },
   "source": [
    "## 3. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_trainer"
   },
   "outputs": [],
   "source": [
    "# Initialize GRPO trainer\n",
    "print(\"üöÄ Initializing Diplomacy GRPO Trainer...\")\n",
    "trainer = DiplomacyGRPOTrainer(config)\n",
    "print(\"‚úÖ Trainer initialized successfully!\")\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in trainer.model.parameters())\n",
    "trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel Info:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / 1e9:.1f} GB (fp32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_episode"
   },
   "source": [
    "## 4. Test Single Episode (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_test_episode"
   },
   "outputs": [],
   "source": [
    "# Run a single episode to test the system\n",
    "print(\"üéÆ Running test episode...\")\n",
    "\n",
    "# Run one episode\n",
    "episode_result = trainer.run_episode()\n",
    "\n",
    "# Print results\n",
    "stats = episode_result['stats']\n",
    "alliance_analysis = episode_result['alliance_analysis']\n",
    "\n",
    "print(\"\\nüìä Episode Results:\")\n",
    "print(f\"  Winner: {stats['winner']}\")\n",
    "print(f\"  Game Length: {stats['game_length_phases']} phases\")\n",
    "print(f\"  Total Steps: {stats['total_steps']}\")\n",
    "print(f\"  Average Step Reward: {stats['avg_step_reward']:.2f}\")\n",
    "\n",
    "print(\"\\nü§ù Alliance Analysis:\")\n",
    "print(f\"  Alliances Formed: {alliance_analysis['total_alliances_formed']}\")\n",
    "print(f\"  Alliances Broken: {alliance_analysis['alliances_broken']}\")\n",
    "print(f\"  Betrayals Detected: {alliance_analysis['betrayals_detected']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Test episode completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 5. Full Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_logging"
   },
   "outputs": [],
   "source": [
    "# Setup advanced logging and monitoring\n",
    "import wandb\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Weights & Biases (optional)\n",
    "try:\n",
    "    wandb.init(\n",
    "        project=\"diplomacy-grpo\",\n",
    "        config=vars(config),\n",
    "        name=f\"grpo-qwen2.5-1.5b-{config.num_episodes}ep\"\n",
    "    )\n",
    "    use_wandb = True\n",
    "    print(\"üìä Weights & Biases initialized\")\n",
    "except:\n",
    "    use_wandb = False\n",
    "    print(\"üìà Using local logging only\")\n",
    "\n",
    "# Training monitoring\n",
    "training_metrics = {\n",
    "    'episode_rewards': [],\n",
    "    'game_lengths': [],\n",
    "    'alliance_counts': [],\n",
    "    'victory_distribution': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_training",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Main training loop with progress monitoring\n",
    "print(f\"üèÅ Starting GRPO training for {config.num_episodes} episodes...\")\n",
    "print(f\"‚è±Ô∏è Estimated time: ~{config.num_episodes * 30:.0f} minutes\\n\")\n",
    "\n",
    "try:\n",
    "    for episode in range(config.num_episodes):\n",
    "        print(f\"\\nüéÆ Episode {episode + 1}/{config.num_episodes}\")\n",
    "        \n",
    "        # Run episode\n",
    "        episode_result = trainer.run_episode()\n",
    "        \n",
    "        # Update model with GRPO\n",
    "        trainer.update_model(episode_result)\n",
    "        \n",
    "        # Extract metrics\n",
    "        stats = episode_result['stats']\n",
    "        alliance_analysis = episode_result['alliance_analysis']\n",
    "        \n",
    "        # Store metrics\n",
    "        training_metrics['episode_rewards'].append(np.mean(stats['final_rewards']))\n",
    "        training_metrics['game_lengths'].append(stats['game_length_phases'])\n",
    "        training_metrics['alliance_counts'].append(alliance_analysis['total_alliances_formed'])\n",
    "        training_metrics['victory_distribution'].append(stats['winner'])\n",
    "        \n",
    "        # Log to wandb if available\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                'episode': episode + 1,\n",
    "                'avg_reward': np.mean(stats['final_rewards']),\n",
    "                'game_length': stats['game_length_phases'],\n",
    "                'alliances_formed': alliance_analysis['total_alliances_formed'],\n",
    "                'alliances_broken': alliance_analysis['alliances_broken'],\n",
    "                'winner': stats['winner']\n",
    "            })\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"  Winner: {stats['winner']}, Length: {stats['game_length_phases']} phases\")\n",
    "        print(f\"  Avg Reward: {np.mean(stats['final_rewards']):.2f}\")\n",
    "        print(f\"  Alliances: {alliance_analysis['total_alliances_formed']} formed, {alliance_analysis['alliances_broken']} broken\")\n",
    "        \n",
    "        # Checkpoint saving\n",
    "        if (episode + 1) % config.save_every == 0:\n",
    "            print(f\"üíæ Saving checkpoint at episode {episode + 1}...\")\n",
    "            trainer.save_checkpoint(episode + 1)\n",
    "        \n",
    "        # Progress visualization every 10 episodes\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Plot training progress\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "            \n",
    "            # Rewards over time\n",
    "            axes[0,0].plot(training_metrics['episode_rewards'])\n",
    "            axes[0,0].set_title('Average Episode Rewards')\n",
    "            axes[0,0].set_xlabel('Episode')\n",
    "            axes[0,0].set_ylabel('Reward')\n",
    "            \n",
    "            # Game lengths\n",
    "            axes[0,1].plot(training_metrics['game_lengths'])\n",
    "            axes[0,1].set_title('Game Lengths (Phases)')\n",
    "            axes[0,1].set_xlabel('Episode')\n",
    "            axes[0,1].set_ylabel('Phases')\n",
    "            \n",
    "            # Alliance formation\n",
    "            axes[1,0].plot(training_metrics['alliance_counts'])\n",
    "            axes[1,0].set_title('Alliances Formed per Game')\n",
    "            axes[1,0].set_xlabel('Episode')\n",
    "            axes[1,0].set_ylabel('Count')\n",
    "            \n",
    "            # Victory distribution\n",
    "            victory_counts = {}\n",
    "            for winner in training_metrics['victory_distribution']:\n",
    "                victory_counts[winner] = victory_counts.get(winner, 0) + 1\n",
    "            axes[1,1].bar(victory_counts.keys(), victory_counts.values())\n",
    "            axes[1,1].set_title('Victory Distribution')\n",
    "            axes[1,1].set_xlabel('Power')\n",
    "            axes[1,1].set_ylabel('Wins')\n",
    "            axes[1,1].tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\nüìà Training Progress - Episode {episode + 1}:\")\n",
    "            print(f\"  Latest avg reward: {training_metrics['episode_rewards'][-1]:.2f}\")\n",
    "            print(f\"  Latest game length: {training_metrics['game_lengths'][-1]} phases\")\n",
    "            print(f\"  Victory distribution: {victory_counts}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚èπÔ∏è Training interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\nüéâ Training completed successfully!\")\n",
    "finally:\n",
    "    # Save final results\n",
    "    print(\"üíæ Saving final results...\")\n",
    "    trainer.save_final_results()\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.finish()\n",
    "    \n",
    "    print(\"‚úÖ All results saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 6. Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_analysis"
   },
   "outputs": [],
   "source": [
    "# Analyze training results\n",
    "print(\"üìä Final Training Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Overall statistics\n",
    "total_episodes = len(training_metrics['episode_rewards'])\n",
    "avg_reward = np.mean(training_metrics['episode_rewards'])\n",
    "avg_game_length = np.mean(training_metrics['game_lengths'])\n",
    "avg_alliances = np.mean(training_metrics['alliance_counts'])\n",
    "\n",
    "print(f\"Episodes Completed: {total_episodes}\")\n",
    "print(f\"Average Reward: {avg_reward:.2f}\")\n",
    "print(f\"Average Game Length: {avg_game_length:.1f} phases\")\n",
    "print(f\"Average Alliances per Game: {avg_alliances:.1f}\")\n",
    "\n",
    "# Learning progress\n",
    "if total_episodes >= 20:\n",
    "    early_rewards = np.mean(training_metrics['episode_rewards'][:10])\n",
    "    late_rewards = np.mean(training_metrics['episode_rewards'][-10:])\n",
    "    improvement = late_rewards - early_rewards\n",
    "    \n",
    "    print(f\"\\nLearning Progress:\")\n",
    "    print(f\"  Early episodes (1-10): {early_rewards:.2f}\")\n",
    "    print(f\"  Late episodes (-10): {late_rewards:.2f}\")\n",
    "    print(f\"  Improvement: {improvement:+.2f} ({improvement/early_rewards*100:+.1f}%)\")\n",
    "\n",
    "# Victory distribution analysis\n",
    "victory_counts = {}\n",
    "for winner in training_metrics['victory_distribution']:\n",
    "    victory_counts[winner] = victory_counts.get(winner, 0) + 1\n",
    "\n",
    "print(f\"\\nVictory Distribution:\")\n",
    "for power, wins in sorted(victory_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = wins / total_episodes * 100\n",
    "    print(f\"  {power}: {wins} wins ({percentage:.1f}%)\")\n",
    "\n",
    "# Check for balanced play\n",
    "win_variance = np.var(list(victory_counts.values()))\n",
    "if win_variance < 2.0:\n",
    "    print(\"\\n‚úÖ Victory distribution is well-balanced (low variance)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Victory distribution shows some imbalance (high variance)\")\n",
    "\n",
    "print(\"\\nüéØ Training complete! Check /content/checkpoints for saved models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "testing"
   },
   "source": [
    "## 7. Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_trained_model"
   },
   "outputs": [],
   "source": [
    "# Test the trained model against the original\n",
    "print(\"üÜö Testing trained model vs baseline...\")\n",
    "\n",
    "# Load original model for comparison\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "baseline_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "# Compare models on a simple diplomacy prompt\n",
    "test_prompt = \"\"\"\n",
    "You are playing as FRANCE in Diplomacy. It's Spring 1901. \n",
    "Your current units: A MAR, A PAR, F BRE\n",
    "Possible orders: A MAR-SPA, A MAR-BUR, A MAR H, A PAR-BUR, A PAR-PIC, A PAR H, F BRE-MAO, F BRE-ENG, F BRE H\n",
    "\n",
    "What are your orders?\n",
    "\"\"\"\n",
    "\n",
    "# Generate with both models\n",
    "inputs = trainer.tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "# Trained model response\n",
    "with torch.no_grad():\n",
    "    trained_output = trainer.model.generate(\n",
    "        **inputs, max_new_tokens=100, temperature=0.7, do_sample=True\n",
    "    )\n",
    "trained_response = trainer.tokenizer.decode(\n",
    "    trained_output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# Baseline model response\n",
    "with torch.no_grad():\n",
    "    baseline_output = baseline_model.generate(\n",
    "        **inputs, max_new_tokens=100, temperature=0.7, do_sample=True\n",
    "    )\n",
    "baseline_response = trainer.tokenizer.decode(\n",
    "    baseline_output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"\\nü§ñ Baseline Model Response:\")\n",
    "print(baseline_response)\n",
    "\n",
    "print(\"\\nüß† Trained Model Response:\")\n",
    "print(trained_response)\n",
    "\n",
    "print(\"\\nüìù Note: Look for differences in strategic thinking, order format, and diplomatic language.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export"
   },
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_results"
   },
   "outputs": [],
   "source": [
    "# Prepare files for download\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì¶ Preparing results for download...\")\n",
    "\n",
    "# Create results archive\n",
    "!zip -r diplomacy_grpo_results.zip /content/checkpoints/\n",
    "\n",
    "# Summary report\n",
    "summary_report = f\"\"\"\n",
    "# Diplomacy GRPO Training Results\n",
    "\n",
    "## Configuration\n",
    "- Model: {config.model_name}\n",
    "- Episodes: {total_episodes}\n",
    "- Learning Rate: {config.learning_rate}\n",
    "- Max Year: {config.max_year}\n",
    "\n",
    "## Results\n",
    "- Average Reward: {avg_reward:.2f}\n",
    "- Average Game Length: {avg_game_length:.1f} phases\n",
    "- Average Alliances: {avg_alliances:.1f}\n",
    "\n",
    "## Victory Distribution\n",
    "\"\"\"\n",
    "\n",
    "for power, wins in sorted(victory_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = wins / total_episodes * 100\n",
    "    summary_report += f\"- {power}: {wins} wins ({percentage:.1f}%)\\n\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "\n",
    "## Training Metrics\n",
    "- Win Variance: {win_variance:.2f}\n",
    "- Model Parameters: {total_params:,}\n",
    "\n",
    "## Files\n",
    "- Final model: checkpoints/final_results/final_model/\n",
    "- Training stats: checkpoints/final_results/complete_training_stats.json\n",
    "- Checkpoints: checkpoints/checkpoint_episode_*/\n",
    "\"\"\"\n",
    "\n",
    "# Save summary\n",
    "with open('/content/training_summary.md', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(\"\\nüìä Training Summary:\")\n",
    "print(summary_report)\n",
    "\n",
    "print(\"\\nüíæ Download files:\")\n",
    "print(\"- diplomacy_grpo_results.zip (all checkpoints and models)\")\n",
    "print(\"- training_summary.md (summary report)\")\n",
    "\n",
    "# Download files\n",
    "files.download('diplomacy_grpo_results.zip')\n",
    "files.download('/content/training_summary.md')\n",
    "\n",
    "print(\"\\n‚úÖ Export complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "### Immediate Improvements:\n",
    "1. **Increase Training Scale**: Run for 200+ episodes\n",
    "2. **Longer Games**: Increase `max_year` to 1910 for full games\n",
    "3. **More Negotiations**: Increase `num_negotiation_rounds` to 5+\n",
    "4. **Hyperparameter Tuning**: Experiment with learning rates, KL coefficients\n",
    "\n",
    "### Advanced Features:\n",
    "1. **Population-Based Training**: Train multiple model variants\n",
    "2. **Curriculum Learning**: Start with simpler scenarios\n",
    "3. **Opponent Diversity**: Mix with rule-based or other LLM agents\n",
    "4. **Reward Shaping**: Fine-tune alliance and victory rewards\n",
    "\n",
    "### Integration:\n",
    "1. **Deploy to Game**: Integrate trained model back into the original game\n",
    "2. **Evaluation**: Test against original LLM agents\n",
    "3. **Human Testing**: Play against human players\n",
    "4. **Tournament Mode**: Multi-model competitions\n",
    "\n",
    "### Research Extensions:\n",
    "1. **Multi-Objective RL**: Balance winning vs. diplomatic behavior\n",
    "2. **Transfer Learning**: Apply to other negotiation games\n",
    "3. **Interpretability**: Analyze learned diplomatic strategies\n",
    "4. **Scalability**: Train larger models (7B, 14B parameters)\n",
    "\n",
    "üéØ **Proof of Concept Complete!** This notebook demonstrates that online GRPO training for Diplomacy agents is feasible and effective."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
