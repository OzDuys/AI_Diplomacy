{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Diplomacy GRPO Training with Qwen2.5-1.5B-Instruct\n",
    "\n",
    "This notebook implements online GRPO (Group Relative Policy Optimization) training for Diplomacy agents using the multi-turn framework from willccbb/verifiers.\n",
    "\n",
    "## Features:\n",
    "- **7-Agent Self-Play** with Qwen2.5-1.5B-Instruct\n",
    "- **Online Training** - RL agent learns by playing games\n",
    "- **Alliance Formation Rewards** - Diplomatic success metrics\n",
    "- **Batched Generation** - Efficient GPU utilization\n",
    "- **Full Game Episodes** - Complete Diplomacy games (1901-1910)\n",
    "\n",
    "**Requirements**: Colab Pro (24GB GPU memory recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "**Important**: You need to replace `YOUR_USERNAME` with your actual GitHub username in the git clone command below, or upload the AI_Diplomacy folder directly to Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install all required dependencies\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "\n",
    "# Core ML packages\n",
    "!pip install -q torch transformers accelerate datasets numpy scipy\n",
    "!pip install -q tensorboard wandb matplotlib seaborn\n",
    "\n",
    "# Try to install Flash Attention 2 for memory efficiency (optional)\n",
    "print(\"üîß Attempting to install Flash Attention 2 for memory efficiency...\")\n",
    "try:\n",
    "    !pip install -q flash-attn --no-build-isolation\n",
    "    print(\"‚úÖ Flash Attention 2 installed successfully\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Flash Attention 2 installation failed - will use default attention\")\n",
    "    print(\"   This is normal in some environments and won't affect functionality\")\n",
    "\n",
    "# GRPO framework\n",
    "!pip install -q git+https://github.com/willccbb/verifiers.git\n",
    "\n",
    "# AI Diplomacy specific dependencies\n",
    "!pip install -q coloredlogs python-dotenv ujson tornado tqdm\n",
    "!pip install -q anthropic openai google-generativeai together\n",
    "!pip install -q json-repair json5 bcrypt pytest pylint\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")\n",
    "\n",
    "print(\"\\nüîÑ Cloning AI Diplomacy repository...\")\n",
    "# Clone AI Diplomacy repository (replace with your actual repo URL)\n",
    "!git clone https://github.com/OzDuys/AI_Diplomacy.git\n",
    "%cd AI_Diplomacy\n",
    "\n",
    "print(\"üì¶ Installing AI Diplomacy package...\")\n",
    "# Install in development mode\n",
    "!pip install -q -e .\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Verify installation and check for issues\n",
    "print(\"üîç Verifying installation...\")\n",
    "\n",
    "# Check critical packages\n",
    "packages_to_check = [\n",
    "    'torch', 'transformers', 'accelerate', 'numpy', \n",
    "    'coloredlogs', 'diplomacy', 'ai_diplomacy'\n",
    "]\n",
    "\n",
    "for package in packages_to_check:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"‚úÖ {package} - OK\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå {package} - MISSING: {e}\")\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"\\nüñ•Ô∏è Hardware Check:\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    if torch.cuda.get_device_properties(0).total_memory / 1e9 < 15:\n",
    "        print(\"‚ö†Ô∏è Warning: Less than 15GB GPU memory. Consider using Colab Pro.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Training will be very slow on CPU.\")\n",
    "\n",
    "# Check if we're in the right directory\n",
    "import os\n",
    "if os.path.exists('ai_diplomacy'):\n",
    "    print(\"‚úÖ AI_Diplomacy directory found\")\n",
    "else:\n",
    "    print(\"‚ùå AI_Diplomacy directory not found. Check git clone step.\")\n",
    "\n",
    "print(\"\\nüîß If you see any MISSING packages above, re-run the installation cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 2. Setup API Keys and Environment\n",
    "\n",
    "Let's configure the API keys from Colab secrets and set up the environment properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_config"
   },
   "outputs": [],
   "source": [
    "# Setup environment variables from Colab secrets\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "print(\"üîë Setting up API keys...\")\n",
    "\n",
    "# Set up API keys from Colab secrets\n",
    "try:\n",
    "    # For Qwen2.5-1.5B-Instruct, we'll use it locally, but set up keys just in case\n",
    "    openrouter_key = userdata.get('OPENROUTER_API_KEY')\n",
    "    os.environ['OPENROUTER_API_KEY'] = openrouter_key\n",
    "    print(\"‚úÖ OPENROUTER_API_KEY - Set from Colab secrets\")\n",
    "\n",
    "    # W&B secret\n",
    "    wandb_key = userdata.get('WANDB_API_KEY')\n",
    "    os.environ['WANDB_API_KEY'] = wandb_key\n",
    "    print(\"‚úÖ WANDB_API_KEY - Set from Colab secrets\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è OPENROUTER_API_KEY not found in secrets: {e}\")\n",
    "    print(\"   This is OK for local model usage, but may cause issues if calling external APIs\")\n",
    "\n",
    "# Optional: Set up other API keys if available\n",
    "optional_keys = ['OPENAI_API_KEY', 'ANTHROPIC_API_KEY', 'GOOGLE_API_KEY']\n",
    "for key in optional_keys:\n",
    "    try:\n",
    "        value = userdata.get(key)\n",
    "        os.environ[key] = value\n",
    "        print(f\"‚úÖ {key} - Set from Colab secrets\")\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è {key} - Not found (optional)\")\n",
    "\n",
    "# Verify current environment\n",
    "print(f\"\\nüåç Environment Check:\")\n",
    "print(f\"  OPENROUTER_API_KEY: {'‚úÖ Set' if 'OPENROUTER_API_KEY' in os.environ else '‚ùå Missing'}\")\n",
    "print(f\"  Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Create a minimal .env file for the package\n",
    "with open('.env', 'w') as f:\n",
    "    for key in ['OPENROUTER_API_KEY'] + optional_keys:\n",
    "        if key in os.environ:\n",
    "            f.write(f\"{key}={os.environ[key]}\\n\")\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "initialize"
   },
   "source": [
    "## 3. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_trainer"
   },
   "outputs": [],
   "source": [
    "# Test specific imports to diagnose issues\n",
    "print(\"üß™ Testing imports...\")\n",
    "\n",
    "# Test transformers import\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    print(\"‚úÖ transformers - OK\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå transformers - MISSING: {e}\")\n",
    "    print(\"Run: !pip install transformers\")\n",
    "\n",
    "# Test verifiers import\n",
    "try:\n",
    "    import verifiers\n",
    "    print(\"‚úÖ verifiers - OK\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå verifiers - MISSING: {e}\")\n",
    "    print(\"Run: !pip install git+https://github.com/willccbb/verifiers.git\")\n",
    "\n",
    "# Test Flash Attention 2 availability\n",
    "try:\n",
    "    import flash_attn\n",
    "    print(\"‚úÖ Flash Attention 2 - Available\")\n",
    "    flash_attn_available = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Flash Attention 2 - Not available (will use default attention)\")\n",
    "    flash_attn_available = False\n",
    "\n",
    "# Test our modules\n",
    "try:\n",
    "    from ai_diplomacy.grpo_trainer import TrainingConfig, DiplomacyGRPOTrainer\n",
    "    import logging\n",
    "    print(\"‚úÖ Successfully imported GRPO training modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Please ensure all dependencies are installed and the repository is cloned correctly.\")\n",
    "    \n",
    "    # Check what's missing specifically\n",
    "    import sys\n",
    "    print(f\"Python path: {sys.path}\")\n",
    "    import os\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "    print(f\"Directory contents: {os.listdir('.')}\")\n",
    "    \n",
    "    raise\n",
    "\n",
    "# Enhanced configuration to utilize more VRAM (24GB available)\n",
    "# Adjust model size based on Flash Attention availability\n",
    "if flash_attn_available:\n",
    "    model_name = \"Qwen/Qwen2.5-7B-Instruct\"  # Use larger model with Flash Attention\n",
    "    batch_size = 14  # 2 parallel games\n",
    "    max_length = 4096\n",
    "    print(\"üöÄ Using optimized config with Flash Attention 2\")\n",
    "else:\n",
    "    model_name = \"Qwen/Qwen2.5-3B-Instruct\"  # Use smaller model without Flash Attention\n",
    "    batch_size = 10  # Smaller batch size for safety\n",
    "    max_length = 2048\n",
    "    print(\"üîß Using conservative config without Flash Attention 2\")\n",
    "\n",
    "config = TrainingConfig(\n",
    "    # Model settings - optimize for available memory\n",
    "    model_name=model_name,\n",
    "    max_length=max_length,\n",
    "    \n",
    "    # Training settings - adjust based on capabilities\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=1e-5,\n",
    "    num_episodes=50,        # Start with fewer episodes for testing\n",
    "    max_year=1906,          # Shorter games for faster iteration\n",
    "    num_negotiation_rounds=3,  # Reasonable number of rounds\n",
    "    \n",
    "    # GRPO specific\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    kl_coeff=0.1,\n",
    "    num_generations=1,      # Single generation to start\n",
    "    gradient_accumulation_steps=2,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_every=10,\n",
    "    checkpoint_dir=\"/content/checkpoints\",\n",
    "    \n",
    "    # Enhanced W&B Logging with reduced verbosity\n",
    "    log_level=\"WARNING\",    # Reduced from INFO to WARNING for cleaner output\n",
    "    log_alliance_analysis=True,\n",
    "    use_wandb=True,\n",
    "    wandb_project=\"diplomacy-grpo-enhanced\",\n",
    "    log_step_rewards=True,\n",
    "    log_center_changes=True,\n",
    "    log_model_weights=False,  # Disable for initial run\n",
    "    \n",
    "    # Seeds for reproducibility\n",
    "    random_seed=42,\n",
    "    torch_seed=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Enhanced Training Configuration:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Context Length: {config.max_length} tokens\")\n",
    "print(f\"  Batch Size: {config.batch_size}\")\n",
    "print(f\"  Episodes: {config.num_episodes}\")\n",
    "print(f\"  Max Year: {config.max_year}\")\n",
    "print(f\"  Learning Rate: {config.learning_rate}\")\n",
    "print(f\"  Generations per prompt: {config.num_generations}\")\n",
    "print(f\"  Gradient accumulation: {config.gradient_accumulation_steps}\")\n",
    "print(f\"  Log Level: {config.log_level} (reduced verbosity)\")\n",
    "print(f\"  W&B Logging: {config.use_wandb}\")\n",
    "\n",
    "# Check VRAM before initialization\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüñ•Ô∏è GPU Memory Status:\")\n",
    "    print(f\"  Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"  Current usage: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "    print(f\"  Available: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1e9:.1f} GB\")\n",
    "\n",
    "# Initialize trainer (will use appropriate VRAM based on config)\n",
    "print(f\"\\nü§ñ Initializing trainer with {config.model_name} and reduced logging verbosity...\")\n",
    "trainer = DiplomacyGRPOTrainer(config)\n",
    "\n",
    "# Check VRAM after initialization\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüñ•Ô∏è GPU Memory After Model Load:\")\n",
    "    print(f\"  Current usage: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "    print(f\"  Peak usage: {torch.cuda.max_memory_allocated() / 1e9:.1f} GB\")\n",
    "    print(f\"  Available: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1e9:.1f} GB\")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized with optimized VRAM usage and clean logging!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_episode"
   },
   "source": [
    "## 4. Test Single Episode (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_test_episode"
   },
   "outputs": [],
   "source": [
    "# Run a single episode to test the system\n",
    "print(\"üéÆ Running test episode...\")\n",
    "\n",
    "# Run one episode\n",
    "episode_result = trainer.run_episode()\n",
    "\n",
    "# Print results\n",
    "stats = episode_result['stats']\n",
    "alliance_analysis = episode_result['alliance_analysis']\n",
    "\n",
    "print(\"\\nüìä Episode Results:\")\n",
    "print(f\"  Winner: {stats['winner']}\")\n",
    "print(f\"  Game Length: {stats['game_length_phases']} phases\")\n",
    "print(f\"  Total Steps: {stats['total_steps']}\")\n",
    "print(f\"  Average Step Reward: {stats['avg_step_reward']:.2f}\")\n",
    "\n",
    "print(\"\\nü§ù Alliance Analysis:\")\n",
    "print(f\"  Alliances Formed: {alliance_analysis['total_alliances_formed']}\")\n",
    "print(f\"  Alliances Broken: {alliance_analysis['alliances_broken']}\")\n",
    "print(f\"  Betrayals Detected: {alliance_analysis['betrayals_detected']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Test episode completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 5. Full Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_logging"
   },
   "outputs": [],
   "source": [
    "# Setup advanced logging and monitoring with proper field type handling\n",
    "import wandb\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"üìä Setting up Enhanced W&B Logging...\")\n",
    "print(\"üîß Field Type Optimizations:\")\n",
    "print(\"   ‚Ä¢ Converted string fields to numeric for better visualization\")\n",
    "print(\"   ‚Ä¢ Phase tracking: game_year (1901-1910), game_season (0=Spring, 1=Fall, 2=Winter)\")\n",
    "print(\"   ‚Ä¢ Decision type: decision_type_numeric (1=orders, 0=negotiation)\")\n",
    "print(\"   ‚Ä¢ Winners: winner_id (AUSTRIA=0, ENGLAND=1, etc.) + victory flags\")\n",
    "print(\"   ‚Ä¢ Proper metric definitions to avoid media type conflicts\")\n",
    "\n",
    "# Enhanced W&B configuration will be handled by the trainer\n",
    "# This avoids the string field conflicts you encountered\n",
    "\n",
    "# Training monitoring (local backup)\n",
    "training_metrics = {\n",
    "    'episode_rewards': [],\n",
    "    'game_lengths': [],\n",
    "    'alliance_counts': [],\n",
    "    'victory_distribution': []\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Enhanced monitoring setup complete!\")\n",
    "print(\"üí° W&B Dashboard Tips:\")\n",
    "print(\"   ‚Ä¢ Use 'game_year' and 'game_season' for timeline analysis\")\n",
    "print(\"   ‚Ä¢ 'decision_type_numeric' shows orders (1) vs negotiation (0) phases\")\n",
    "print(\"   ‚Ä¢ 'winner_id' and 'victory_*' fields track victories numerically\")\n",
    "print(\"   ‚Ä¢ 'centers_game_*' fields show real-time supply center control\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_training",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Main training loop with comprehensive W&B monitoring and optimized VRAM usage\n",
    "print(f\"üèÅ Starting Enhanced GRPO training for {config.num_episodes} episodes...\")\n",
    "print(f\"üöÄ VRAM Optimizations:\")\n",
    "print(f\"   ‚Ä¢ Model: {config.model_name} (7B parameters)\")\n",
    "print(f\"   ‚Ä¢ Parallel Games: {config.batch_size // 7} simultaneous games\")\n",
    "print(f\"   ‚Ä¢ Context Length: {config.max_length} tokens\")\n",
    "print(f\"   ‚Ä¢ Multiple Generations: {config.num_generations} per prompt\")\n",
    "print(f\"   ‚Ä¢ Gradient Checkpointing: Enabled\")\n",
    "print(f\"   ‚Ä¢ Flash Attention: Auto-detected\")\n",
    "print(f\"‚è±Ô∏è Estimated time: ~{config.num_episodes * 20:.0f} minutes (faster with parallel games)\")\n",
    "print(f\"üìä W&B Project: {config.wandb_project}\")\n",
    "print(f\"üîç Detailed logging: step rewards, center changes, alliances, GRPO updates, model weights\\n\")\n",
    "\n",
    "# Monitor VRAM usage during training\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üíæ Initial VRAM Usage:\")\n",
    "    print(f\"   Current: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "    print(f\"   Peak: {torch.cuda.max_memory_allocated() / 1e9:.1f} GB\")\n",
    "    print(f\"   Available: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1e9:.1f} GB\")\n",
    "    print()\n",
    "\n",
    "try:\n",
    "    # Training loop now handles parallel games and optimized VRAM usage\n",
    "    trainer.train()\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚èπÔ∏è Training interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\nüéâ Training completed successfully!\")\n",
    "finally:\n",
    "    print(\"‚úÖ All results saved and logged to W&B!\")\n",
    "    \n",
    "    # Final VRAM usage\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\nüíæ Final VRAM Usage:\")\n",
    "        print(f\"   Peak: {torch.cuda.max_memory_allocated() / 1e9:.1f} GB\")\n",
    "        print(f\"   Efficiency: {(torch.cuda.max_memory_allocated() / torch.cuda.get_device_properties(0).total_memory) * 100:.1f}% of total VRAM used\")\n",
    "\n",
    "# Display final metrics summary\n",
    "if trainer.training_stats['episode_rewards']:\n",
    "    total_episodes = len(trainer.training_stats['episode_rewards'])\n",
    "    avg_reward = np.mean([np.mean(rewards) for rewards in trainer.training_stats['episode_rewards']])\n",
    "    \n",
    "    print(f\"\\nüìà Final Training Summary:\")\n",
    "    print(f\"  Total Episode Batches: {total_episodes}\")\n",
    "    print(f\"  Total Games Played: {total_episodes * trainer.num_parallel_games}\")\n",
    "    print(f\"  Average Reward: {avg_reward:.2f}\")\n",
    "    print(f\"  Parallel Efficiency: {trainer.num_parallel_games}x speedup\")\n",
    "    print(f\"  W&B Dashboard: https://wandb.ai/{config.wandb_project}\")\n",
    "    \n",
    "    # Victory distribution across all parallel games\n",
    "    victory_counts = {}\n",
    "    for winner in trainer.training_stats['victory_distribution']:\n",
    "        victory_counts[winner] = victory_counts.get(winner, 0) + 1\n",
    "    \n",
    "    print(f\"  Victory Distribution (all games):\")\n",
    "    for power, wins in sorted(victory_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        total_games = total_episodes * trainer.num_parallel_games\n",
    "        percentage = wins / total_games * 100\n",
    "        print(f\"    {power}: {wins} wins ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nüéØ Enhanced W&B Logging Includes:\")\n",
    "print(\"  ‚Ä¢ Step-by-step rewards for all agents across parallel games\")\n",
    "print(\"  ‚Ä¢ Real-time supply center tracking per game\") \n",
    "print(\"  ‚Ä¢ Alliance formation and betrayal detection\")\n",
    "print(\"  ‚Ä¢ GRPO training loss, gradients, and model weights\")\n",
    "print(\"  ‚Ä¢ Parallel game efficiency metrics\")\n",
    "print(\"  ‚Ä¢ Victory distributions and learning trends\")\n",
    "print(\"  ‚Ä¢ VRAM utilization and memory efficiency\")\n",
    "print(\"  ‚Ä¢ Cross-game analysis and aggregate statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 6. Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_analysis"
   },
   "outputs": [],
   "source": [
    "# Analyze training results\n",
    "print(\"üìä Final Training Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Overall statistics\n",
    "total_episodes = len(training_metrics['episode_rewards'])\n",
    "avg_reward = np.mean(training_metrics['episode_rewards'])\n",
    "avg_game_length = np.mean(training_metrics['game_lengths'])\n",
    "avg_alliances = np.mean(training_metrics['alliance_counts'])\n",
    "\n",
    "print(f\"Episodes Completed: {total_episodes}\")\n",
    "print(f\"Average Reward: {avg_reward:.2f}\")\n",
    "print(f\"Average Game Length: {avg_game_length:.1f} phases\")\n",
    "print(f\"Average Alliances per Game: {avg_alliances:.1f}\")\n",
    "\n",
    "# Learning progress\n",
    "if total_episodes >= 20:\n",
    "    early_rewards = np.mean(training_metrics['episode_rewards'][:10])\n",
    "    late_rewards = np.mean(training_metrics['episode_rewards'][-10:])\n",
    "    improvement = late_rewards - early_rewards\n",
    "    \n",
    "    print(f\"\\nLearning Progress:\")\n",
    "    print(f\"  Early episodes (1-10): {early_rewards:.2f}\")\n",
    "    print(f\"  Late episodes (-10): {late_rewards:.2f}\")\n",
    "    print(f\"  Improvement: {improvement:+.2f} ({improvement/early_rewards*100:+.1f}%)\")\n",
    "\n",
    "# Victory distribution analysis\n",
    "victory_counts = {}\n",
    "for winner in training_metrics['victory_distribution']:\n",
    "    victory_counts[winner] = victory_counts.get(winner, 0) + 1\n",
    "\n",
    "print(f\"\\nVictory Distribution:\")\n",
    "for power, wins in sorted(victory_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = wins / total_episodes * 100\n",
    "    print(f\"  {power}: {wins} wins ({percentage:.1f}%)\")\n",
    "\n",
    "# Check for balanced play\n",
    "win_variance = np.var(list(victory_counts.values()))\n",
    "if win_variance < 2.0:\n",
    "    print(\"\\n‚úÖ Victory distribution is well-balanced (low variance)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Victory distribution shows some imbalance (high variance)\")\n",
    "\n",
    "print(\"\\nüéØ Training complete! Check /content/checkpoints for saved models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "testing"
   },
   "source": [
    "## 7. Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_trained_model"
   },
   "outputs": [],
   "source": [
    "# Test the trained model against the original\n",
    "print(\"üÜö Testing trained model vs baseline...\")\n",
    "\n",
    "# Load original model for comparison\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "baseline_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "# Compare models on a simple diplomacy prompt\n",
    "test_prompt = \"\"\"\n",
    "You are playing as FRANCE in Diplomacy. It's Spring 1901. \n",
    "Your current units: A MAR, A PAR, F BRE\n",
    "Possible orders: A MAR-SPA, A MAR-BUR, A MAR H, A PAR-BUR, A PAR-PIC, A PAR H, F BRE-MAO, F BRE-ENG, F BRE H\n",
    "\n",
    "What are your orders?\n",
    "\"\"\"\n",
    "\n",
    "# Generate with both models\n",
    "inputs = trainer.tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "# Trained model response\n",
    "with torch.no_grad():\n",
    "    trained_output = trainer.model.generate(\n",
    "        **inputs, max_new_tokens=100, temperature=0.7, do_sample=True\n",
    "    )\n",
    "trained_response = trainer.tokenizer.decode(\n",
    "    trained_output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# Baseline model response\n",
    "with torch.no_grad():\n",
    "    baseline_output = baseline_model.generate(\n",
    "        **inputs, max_new_tokens=100, temperature=0.7, do_sample=True\n",
    "    )\n",
    "baseline_response = trainer.tokenizer.decode(\n",
    "    baseline_output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"\\nü§ñ Baseline Model Response:\")\n",
    "print(baseline_response)\n",
    "\n",
    "print(\"\\nüß† Trained Model Response:\")\n",
    "print(trained_response)\n",
    "\n",
    "print(\"\\nüìù Note: Look for differences in strategic thinking, order format, and diplomatic language.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export"
   },
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_results"
   },
   "outputs": [],
   "source": [
    "# Prepare files for download\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì¶ Preparing results for download...\")\n",
    "\n",
    "# Create results archive\n",
    "!zip -r diplomacy_grpo_results.zip /content/checkpoints/\n",
    "\n",
    "# Summary report\n",
    "summary_report = f\"\"\"\n",
    "# Diplomacy GRPO Training Results\n",
    "\n",
    "## Configuration\n",
    "- Model: {config.model_name}\n",
    "- Episodes: {total_episodes}\n",
    "- Learning Rate: {config.learning_rate}\n",
    "- Max Year: {config.max_year}\n",
    "\n",
    "## Results\n",
    "- Average Reward: {avg_reward:.2f}\n",
    "- Average Game Length: {avg_game_length:.1f} phases\n",
    "- Average Alliances: {avg_alliances:.1f}\n",
    "\n",
    "## Victory Distribution\n",
    "\"\"\"\n",
    "\n",
    "for power, wins in sorted(victory_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = wins / total_episodes * 100\n",
    "    summary_report += f\"- {power}: {wins} wins ({percentage:.1f}%)\\n\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "\n",
    "## Training Metrics\n",
    "- Win Variance: {win_variance:.2f}\n",
    "- Model Parameters: {total_params:,}\n",
    "\n",
    "## Files\n",
    "- Final model: checkpoints/final_results/final_model/\n",
    "- Training stats: checkpoints/final_results/complete_training_stats.json\n",
    "- Checkpoints: checkpoints/checkpoint_episode_*/\n",
    "\"\"\"\n",
    "\n",
    "# Save summary\n",
    "with open('/content/training_summary.md', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(\"\\nüìä Training Summary:\")\n",
    "print(summary_report)\n",
    "\n",
    "print(\"\\nüíæ Download files:\")\n",
    "print(\"- diplomacy_grpo_results.zip (all checkpoints and models)\")\n",
    "print(\"- training_summary.md (summary report)\")\n",
    "\n",
    "# Download files\n",
    "files.download('diplomacy_grpo_results.zip')\n",
    "files.download('/content/training_summary.md')\n",
    "\n",
    "print(\"\\n‚úÖ Export complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "### Immediate Improvements:\n",
    "1. **Increase Training Scale**: Run for 200+ episodes\n",
    "2. **Longer Games**: Increase `max_year` to 1910 for full games\n",
    "3. **More Negotiations**: Increase `num_negotiation_rounds` to 5+\n",
    "4. **Hyperparameter Tuning**: Experiment with learning rates, KL coefficients\n",
    "\n",
    "### Advanced Features:\n",
    "1. **Population-Based Training**: Train multiple model variants\n",
    "2. **Curriculum Learning**: Start with simpler scenarios\n",
    "3. **Opponent Diversity**: Mix with rule-based or other LLM agents\n",
    "4. **Reward Shaping**: Fine-tune alliance and victory rewards\n",
    "\n",
    "### Integration:\n",
    "1. **Deploy to Game**: Integrate trained model back into the original game\n",
    "2. **Evaluation**: Test against original LLM agents\n",
    "3. **Human Testing**: Play against human players\n",
    "4. **Tournament Mode**: Multi-model competitions\n",
    "\n",
    "### Research Extensions:\n",
    "1. **Multi-Objective RL**: Balance winning vs. diplomatic behavior\n",
    "2. **Transfer Learning**: Apply to other negotiation games\n",
    "3. **Interpretability**: Analyze learned diplomatic strategies\n",
    "4. **Scalability**: Train larger models (7B, 14B parameters)\n",
    "\n",
    "üéØ **Proof of Concept Complete!** This notebook demonstrates that online GRPO training for Diplomacy agents is feasible and effective."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
